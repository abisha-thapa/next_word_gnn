{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 945
    },
    "collapsed": true,
    "id": "-gEF5GApB36G",
    "outputId": "3f845bd6-e39e-47a9-87ec-299f409a1dda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (from wikipedia) (4.11.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (from wikipedia) (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.9.24)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (from beautifulsoup4->wikipedia) (2.3.1)\n",
      "Requirement already satisfied: nltk in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: joblib in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: pyspellchecker in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (0.7.0)\n",
      "Requirement already satisfied: networkx in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (2.8.4)\n",
      "Requirement already satisfied: dgl in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (0.9.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (from dgl) (1.19.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (from dgl) (1.6.2)\n",
      "Requirement already satisfied: networkx>=2.1 in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (from dgl) (2.8.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (from dgl) (2.28.1)\n",
      "Requirement already satisfied: tqdm in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (from dgl) (4.64.1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (from dgl) (5.9.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (from requests>=2.19.0->dgl) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (from requests>=2.19.0->dgl) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (from requests>=2.19.0->dgl) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (from requests>=2.19.0->dgl) (1.26.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia\n",
    "!pip install nltk\n",
    "!pip install pyspellchecker\n",
    "!pip install networkx\n",
    "!pip install dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BT4vp083bI12",
    "outputId": "4ca05136-4785-4cf1-b26d-2122779ea4f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)\n",
      "E: Unable to lock directory /var/lib/apt/lists/\n",
      "W: Problem unlinking the file /var/cache/apt/pkgcache.bin - RemoveCaches (13: Permission denied)\n",
      "W: Problem unlinking the file /var/cache/apt/srcpkgcache.bin - RemoveCaches (13: Permission denied)\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "Requirement already satisfied: pyenchant in /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!apt update\n",
    "!apt install enchant --fix-missing\n",
    "!apt install -qq enchant\n",
    "!pip install pyenchant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6k2U7_VHB1u5",
    "outputId": "f878161d-c692-4ef8-d679-47a2333b3574"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wikipedia\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from spellchecker import SpellChecker\n",
    "import enchant\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pickle\n",
    "import torch\n",
    "import scipy.sparse as sp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GraphConv\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the embedding size and GPU device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIMENSION = 64\n",
    "device = torch.device('cuda')\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BLgFmfoMDw6j",
    "outputId": "03371a9b-7590-40c5-a408-75001809c70c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/admin1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/admin1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a custom Wikipedia corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1Hvw5_wB1u-",
    "outputId": "29a43ed3-7320-44cd-d4f2-782280c3ce99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some Error occured for keyword :  Music\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/admin1/anaconda3/envs/tf_gpu_env/lib/python3.8/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some Error occured for keyword :  Music (disambiguation)\n",
      "Some Error occured for keyword :  Pop music\n",
      "Some Error occured for keyword :  Sony Music\n",
      "Some Error occured for keyword :  K-pop\n",
      "Some Error occured for keyword :  Musical instrument\n",
      "Some Error occured for keyword :  Composer\n",
      "Some Error occured for keyword :  Record producer\n",
      "Some Error occured for keyword :  Garage\n",
      "Some Error occured for keyword :  20th-century music\n",
      "Some Error occured for keyword :  Pop rock\n",
      "Some Error occured for keyword :  Rebel Music\n",
      "Some Error occured for keyword :  Jazz\n",
      "Some Error occured for keyword :  Music group (disambiguation)\n",
      "Some Error occured for keyword :  American Music Awards of 2002\n",
      "Some Error occured for keyword :  Mode (music)\n",
      "Some Error occured for keyword :  Roots music\n",
      "Some Error occured for keyword :  English music\n",
      "Some Error occured for keyword :  Musical notation\n",
      "Some Error occured for keyword :  Music player\n",
      "Some Error occured for keyword :  Techno\n",
      "Some Error occured for keyword :  Roxy Music\n",
      "Some Error occured for keyword :  MAMA Awards\n",
      "Some Error occured for keyword :  EMI\n",
      "Some Error occured for keyword :  Score\n",
      "Some Error occured for keyword :  Live from Radio City Music Hall\n",
      "Some Error occured for keyword :  Juke music\n",
      "Some Error occured for keyword :  Impressionism in music\n",
      "Some Error occured for keyword :  Fantasia (music)\n",
      "Some Error occured for keyword :  Swamp music\n",
      "Some Error occured for keyword :  Battle Music\n",
      "Some Error occured for keyword :  Canned music\n",
      "Some Error occured for keyword :  Dance\n",
      "Some Error occured for keyword :  Gypsy music\n",
      "Some Error occured for keyword :  Court music\n",
      "Some Error occured for keyword :  American Music Awards of 2005\n",
      "Some Error occured for keyword :  Western music\n",
      "Some Error occured for keyword :  Electronic music (disambiguation)\n",
      "Some Error occured for keyword :  Trance music\n",
      "Some Error occured for keyword :  Samba\n",
      "Some Error occured for keyword :  1490s in music\n",
      "Some Error occured for keyword :  Music (2021 film)\n",
      "Some Error occured for keyword :  Banda music\n",
      "Some Error occured for keyword :  Rocketman (film)\n",
      "Some Error occured for keyword :  Lawrence Welk\n",
      "Some Error occured for keyword :  Beat music\n",
      "Some Error occured for keyword :  American Music Awards of 2004\n",
      "Some Error occured for keyword :  Hardcore\n",
      "Some Error occured for keyword :  Capitol Music Group\n",
      "Some Error occured for keyword :  Now That's What I Call Music! (disambiguation)\n",
      "Some Error occured for keyword :  American Music Awards of 2009\n",
      "Some Error occured for keyword :  Soundtrack\n",
      "Some Error occured for keyword :  Sajid–Wajid\n",
      "Some Error occured for keyword :  Oriental music\n",
      "Some Error occured for keyword :  Star Maa\n",
      "Some Error occured for keyword :  Music FM\n"
     ]
    }
   ],
   "source": [
    "# The variable \"keyword\" decides what type of articles to pull from wikipedia\n",
    "# We tested our approach on the following keywords: sports, celebrity, music\n",
    "\n",
    "keyword = \"music\"\n",
    "corpus_file = \"data/\" + keyword + \"_corpus.pk\"\n",
    "corpus = \"\"\n",
    "\n",
    "if os.path.isfile(corpus_file):\n",
    "    corpus = pickle.load(open(corpus_file, \"rb\"))\n",
    "else:\n",
    "\n",
    "    search_results = wikipedia.search(keyword, results=1000)\n",
    "\n",
    "    for result in search_results:\n",
    "        try:\n",
    "            summary = wikipedia.summary(result)\n",
    "            corpus += summary\n",
    "        except:\n",
    "            print(\"Some Error occured for keyword : \", result)\n",
    "        finally:\n",
    "            pass\n",
    "    pickle.dump(corpus, open(corpus_file, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process the text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qsN-RqXUB1vA"
   },
   "outputs": [],
   "source": [
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())\n",
    " \n",
    "def remove_stopwords(text_tokens):\n",
    "    result = []\n",
    "    en_stopwords = stopwords.words('english')\n",
    "    \n",
    "    for token in text_tokens:\n",
    "        if token not in en_stopwords:\n",
    "            result.append(token)\n",
    "\n",
    "    return result\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    lst=tokenizer.tokenize(' '.join(text))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NDcLe8MDB1vB"
   },
   "outputs": [],
   "source": [
    "def pre_process_text(text_corpus):\n",
    "    #Step 1: Convert every word to lower case to avoid any ambiguity\n",
    "    lower_text_corpus = text_corpus.lower()\n",
    "    \n",
    "    #Step 2: Remove extra whitespaces\n",
    "    lower_text_corpus = remove_whitespace(lower_text_corpus)\n",
    "    \n",
    "    #Step 3: Tokenize in word level\n",
    "    tokens = word_tokenize(lower_text_corpus)\n",
    "    \n",
    "    #Step 4: Remove the stopwords\n",
    "#     tokens = remove_stopwords(tokens)\n",
    "    \n",
    "    #Step 5: Remove punctuations\n",
    "    tokens = remove_punctuations(tokens)\n",
    "    return \" \".join(tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Music! Music! Music! (Put Another Nickel In)\" is a popular song written by Stephen Weiss and Bernie Baum and published in 1950.\"Music! Music! Music! (Put Another Nickel In)\" is a popular song written by Stephen Weiss and Bernie Baum and published in 1950.Country (also called country and western) is a genre of popular music that originated with blues, church music such as Southern gospel and spirituals, old-time, and American folk music forms including Appalachian, Cajun, Creole, Hawaiian, and the cowboy Western music styles of New Mexico, Red Dirt, Tejano, and Texas country. Its popularized roots originate in the Southern and Southwestern United States of the early 1920s.\\nCountry music often consists of ballads and honky-tonk dance tunes with generally simple form, folk lyrics, and harmonies often accompanied by string instruments such as electric and acoustic guitars, steel guitars (such as pedal steels and dobros), banjos, and fiddles as well as harmonicas. Blues modes have been use'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "0pE_jIJSB1vB"
   },
   "outputs": [],
   "source": [
    "processed_corpus = pre_process_text(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'music music music put another nickel in is a popular song written by stephen weiss and bernie baum and published in 1950 music music music put another nickel in is a popular song written by stephen weiss and bernie baum and published in 1950 country also called country and western is a genre of popular music that originated with blues church music such as southern gospel and spirituals old time and american folk music forms including appalachian cajun creole hawaiian and the cowboy western music styles of new mexico red dirt tejano and texas country its popularized roots originate in the southern and southwestern united states of the early 1920s country music often consists of ballads and honky tonk dance tunes with generally simple form folk lyrics and harmonies often accompanied by string instruments such as electric and acoustic guitars steel guitars such as pedal steels and dobros banjos and fiddles as well as harmonicas blues modes have been used extensively throughout its recorde'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_corpus[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76600"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_corpus.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct lookup for word-to-index and index-to-word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "bDB7ZVOVUnhY"
   },
   "outputs": [],
   "source": [
    "# word_count_map = dict()\n",
    "word_index = dict()\n",
    "index_word = dict()\n",
    "processed_word_tokens = processed_corpus.split(\" \")\n",
    "unique_words = list(set(processed_word_tokens))\n",
    "\n",
    "for idx, word in enumerate(unique_words):\n",
    "    word_index[word] = idx\n",
    "    index_word[idx] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q_4cGurpVTOh",
    "outputId": "66bf51ae-f56f-4624-860f-b7b583e38cf7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9344"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a single large graph for the pre-processed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "GbD22rEQMmqX"
   },
   "outputs": [],
   "source": [
    "src = []\n",
    "dst = []\n",
    "\n",
    "for idx, word in enumerate(processed_word_tokens):\n",
    "    if idx < (len(processed_word_tokens)-1):\n",
    "        src.append(word_index[word])\n",
    "        dst.append(word_index[processed_word_tokens[idx+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "1GbxJZn4OaJl"
   },
   "outputs": [],
   "source": [
    "dgl_graph = dgl.DGLGraph()\n",
    "dgl_graph.add_edges(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-qxxTebPFDT",
    "outputId": "6cac297b-c586-4c8b-d5aa-a8bda01321dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 9344 nodes.\n",
      "We have 76599 edges.\n"
     ]
    }
   ],
   "source": [
    "print('We have %d nodes.' % dgl_graph.number_of_nodes())\n",
    "print('We have %d edges.' % dgl_graph.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the vector embeddings for each node in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = torch.nn.Embedding(len(unique_words), EMBEDDING_DIMENSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_emb = torch.LongTensor([range(0,len(unique_words),1)])\n",
    "node_embeddings = None\n",
    "with torch.no_grad():\n",
    "    node_embeddings = embedding_layer(input_emb).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9344, 64])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Graph dataset as a DGLDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikipediaCorpusGraphDataset(dgl.data.DGLDataset):\n",
    "    def __init__(self, src, dst, features):\n",
    "        self.src = src\n",
    "        self.dst = dst\n",
    "        self.features = features\n",
    "        self.graph = None\n",
    "        super().__init__(name=\"wikipedia_corpus\")\n",
    "    \n",
    "    def process(self):\n",
    "        self.graph = dgl.graph((self.src, self.dst))\n",
    "        self.graph.ndata['feat'] = self.features\n",
    "        \n",
    "    def edges(self):\n",
    "        return self.src, self.dst\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Node Embeddings as Edge Prediction on Graph Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dataset = WikipediaCorpusGraphDataset(src, dst, node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76599"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = graph_dataset.graph\n",
    "graph.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v = graph.edges()\n",
    "\n",
    "eids = np.arange(graph.number_of_edges())\n",
    "eids = np.random.permutation(eids)\n",
    "\n",
    "test_size = int(len(eids) * 0.5)\n",
    "train_size = graph.number_of_edges() - test_size\n",
    "\n",
    "test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
    "train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\n",
    "adj_neg = 1 - adj.todense() - np.eye(graph.number_of_nodes())\n",
    "neg_u, neg_v = np.where(adj_neg != 0)\n",
    "\n",
    "neg_eids = np.random.choice(len(neg_u), graph.number_of_edges())\n",
    "test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]\n",
    "train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_g = dgl.remove_edges(graph, eids[:test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=graph.number_of_nodes())\n",
    "train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=graph.number_of_nodes())\n",
    "\n",
    "test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=graph.number_of_nodes())\n",
    "test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=graph.number_of_nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the GCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNStack(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_layers=2, dropout=0.25):\n",
    "        super(GCNStack, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(self.build_conv_model(in_feats, h_feats))\n",
    "        \n",
    "        for i in range(num_layers-1):\n",
    "            self.convs.append(self.build_conv_model(h_feats, h_feats))\n",
    "            \n",
    "        self.num_layers = len(self.convs)\n",
    "        self.lns = nn.ModuleList()\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.lns.append(nn.LayerNorm(h_feats))\n",
    "        \n",
    "        self.dropout = dropout\n",
    "           \n",
    "    def build_conv_model(self, input_dim, hidden_dim):\n",
    "        return GraphConv(input_dim, hidden_dim, allow_zero_in_degree=True)\n",
    "    \n",
    "    def forward(self, graph, input_features):\n",
    "        x = input_features\n",
    "        embeddings = None\n",
    "        \n",
    "        for i, conv_layer in enumerate(self.convs):\n",
    "            x = conv_layer(graph, x)\n",
    "            x = F.relu(x)\n",
    "            embeddings = x\n",
    "            x = F.dropout(x, p=self.dropout, training = self.training)\n",
    "            \n",
    "            if not i == self.num_layers-1:\n",
    "                x = self.lns[i](x)\n",
    "            \n",
    "        return embeddings\n",
    "    \n",
    "    def loss(self, pred, label):\n",
    "        return F.nll_loss(pred, label)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(pos_score, neg_score):\n",
    "#     print(\"Compute LOSS :: Positive Scores :: \", pos_score)\n",
    "#     print(\"Compute LOSS :: Negative Scores :: \", neg_score)\n",
    "    scores = torch.cat([pos_score.float(), neg_score.float()])\n",
    "    pos_labels = []\n",
    "    neg_labels = []\n",
    "    for i in range(pos_score.shape[0]):\n",
    "#         pos_labels.append([1,0])\n",
    "        pos_labels.append([1])\n",
    "    for i in range(neg_score.shape[0]):\n",
    "#         neg_labels.append([0,1])\n",
    "        neg_labels.append([0])\n",
    "    labels = torch.cat([torch.Tensor(pos_labels), torch.Tensor(neg_labels)]).to(device)\n",
    "#     print(\"Scores = \", scores)\n",
    "#     print(\"Labels = \", labels)\n",
    "    \n",
    "#     return F.binary_cross_entropy_with_logits(scores, labels)\n",
    "    return criterion(scores, labels)\n",
    "\n",
    "def compute_auc(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score.float(), neg_score.float()])\n",
    "    pos_labels = []\n",
    "    neg_labels = []\n",
    "    for i in range(pos_score.shape[0]):\n",
    "#         pos_labels.append([1,0])\n",
    "        pos_labels.append([1])\n",
    "    for i in range(neg_score.shape[0]):\n",
    "#         neg_labels.append([0,1])\n",
    "        neg_labels.append([0])\n",
    "    labels = torch.cat([torch.Tensor(pos_labels), torch.Tensor(neg_labels)]).to(device)\n",
    "    return roc_auc_score(labels.detach().cpu().numpy(), scores.detach().cpu().numpy())\n",
    "\n",
    "def compute_ap(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score.float(), neg_score.float()])\n",
    "    pos_labels = []\n",
    "    neg_labels = []\n",
    "    for i in range(pos_score.shape[0]):\n",
    "#         pos_labels.append([1,0])\n",
    "        pos_labels.append([1])\n",
    "    for i in range(neg_score.shape[0]):\n",
    "#         neg_labels.append([0,1])\n",
    "        neg_labels.append([0])\n",
    "    labels = torch.cat([torch.Tensor(pos_labels), torch.Tensor(neg_labels)]).to(device)\n",
    "    return average_precision_score(labels.detach().cpu().numpy(), scores.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_edge_classification(nc_model,linear_pred_edges, dgl_dataset, epochs=100):\n",
    "    optimizer = torch.optim.Adam(nc_model.parameters(), lr=0.01)\n",
    "    optimizer_linear_pred = torch.optim.Adam(linear_pred_edges[0].parameters(), lr=0.01)\n",
    "    \n",
    "    nc_model = nc_model.to(device)\n",
    "    linear_pred_edges = linear_pred_edges.to(device)\n",
    "    \n",
    "    nc_model.train()\n",
    "    linear_pred_edges.train()\n",
    "    \n",
    "    graph = dgl_dataset.graph.to(device)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "\n",
    "    features = graph.ndata['feat']\n",
    "    node_embeddings = None\n",
    "    edges_src, edges_dst = graph.edges()\n",
    "    train_pos_edges_src, train_pos_edges_dst = train_pos_g.edges()\n",
    "    train_neg_edges_src, train_neg_edges_dst = train_neg_g.edges()\n",
    "    \n",
    "    test_pos_edges_src, test_pos_edges_dst = test_pos_g.edges()\n",
    "    test_neg_edges_src, test_neg_edges_dst = test_neg_g.edges()\n",
    "    \n",
    "    for e in tqdm(range(epochs)):\n",
    "        num_correct = 0\n",
    "        num_tests = 0\n",
    "        \n",
    "        node_embeddings = nc_model(graph, features)\n",
    "\n",
    "        edge_embeddings_pos = (node_embeddings[train_pos_edges_src] * node_embeddings[train_pos_edges_dst])/2\n",
    "        edge_embeddings_neg = (node_embeddings[train_neg_edges_src] * node_embeddings[train_neg_edges_dst])/2\n",
    "        \n",
    "        test_edge_embeddings_pos = (node_embeddings[test_pos_edges_src] * node_embeddings[test_pos_edges_dst])/2\n",
    "        test_edge_embeddings_neg = (node_embeddings[test_neg_edges_src] * node_embeddings[test_neg_edges_dst])/2\n",
    "    \n",
    "        pred_edges_pos = linear_pred_edges(edge_embeddings_pos).to(device)\n",
    "        pred_edges_neg = linear_pred_edges(edge_embeddings_neg).to(device)\n",
    "\n",
    "        loss = compute_loss(pred_edges_pos, pred_edges_neg)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_linear_pred.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_linear_pred.step()\n",
    "        \n",
    "        if e % 10 == 0:\n",
    "            print('In epoch {}, loss: {}'.format(e, loss))\n",
    "            print('AUC Score -> ', compute_auc(pred_edges_pos, pred_edges_neg))\n",
    "            print('AP Score  -> ', compute_ap(pred_edges_pos, pred_edges_neg))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pos_score = linear_pred_edges(test_edge_embeddings_pos).to(device)\n",
    "        neg_score = linear_pred_edges(test_edge_embeddings_neg).to(device)\n",
    "        print('Test AUC = ', compute_auc(pos_score, neg_score))\n",
    "        print('Test AP  = ', compute_ap(pos_score, neg_score))\n",
    "        \n",
    "    return node_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Edge Classification / Link Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feats = graph_dataset.graph.ndata['feat'].shape[1]\n",
    "nc_model = GCNStack(num_feats, EMBEDDING_DIMENSION, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                   | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 0.6822222471237183\n",
      "AUC Score ->  0.6762261103422889\n",
      "AP Score  ->  0.7369128088844534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▉                                         | 9/200 [00:01<00:19,  9.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 10, loss: 0.49734437465667725\n",
      "AUC Score ->  0.9245177555917621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▋                                       | 13/200 [00:02<00:21,  8.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP Score  ->  0.9372830845092832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▍                                     | 21/200 [00:02<00:21,  8.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 20, loss: 0.45951351523399353\n",
      "AUC Score ->  0.9418889961074108\n",
      "AP Score  ->  0.9516554808079793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████▎                                   | 30/200 [00:03<00:14, 11.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 30, loss: 0.4162270426750183\n",
      "AUC Score ->  0.9393495170053651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|██████▋                                   | 32/200 [00:04<00:20,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP Score  ->  0.9543930216964814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▍                                 | 40/200 [00:04<00:15, 10.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 40, loss: 0.3654099404811859\n",
      "AUC Score ->  0.9350390117868416\n",
      "AP Score  ->  0.9538003565584503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██████████▎                               | 49/200 [00:05<00:14, 10.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 50, loss: 0.31927812099456787\n",
      "AUC Score ->  0.9405832219866521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|███████████▏                              | 53/200 [00:06<00:17,  8.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP Score  ->  0.9573640289001606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████▌                             | 60/200 [00:06<00:12, 11.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 60, loss: 0.28653910756111145\n",
      "AUC Score ->  0.9526941464595164\n",
      "AP Score  ->  0.9637897036149945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|██████████████▋                           | 70/200 [00:07<00:13,  9.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 70, loss: 0.26090943813323975\n",
      "AUC Score ->  0.9608710690644833\n",
      "AP Score  ->  0.9680528349321953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████                         | 81/200 [00:09<00:12,  9.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 80, loss: 0.24568793177604675\n",
      "AUC Score ->  0.9649054066767107\n",
      "AP Score  ->  0.9704014610602435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|███████████████████                       | 91/200 [00:10<00:11,  9.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 90, loss: 0.2395978569984436\n",
      "AUC Score ->  0.9667778275808001\n",
      "AP Score  ->  0.9713089253074805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████▋                    | 101/200 [00:11<00:11,  8.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 100, loss: 0.23226088285446167\n",
      "AUC Score ->  0.9683986750881115\n",
      "AP Score  ->  0.9725336610838561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|██████████████████████▊                  | 111/200 [00:12<00:10,  8.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 110, loss: 0.23098817467689514\n",
      "AUC Score ->  0.9690164637430209\n",
      "AP Score  ->  0.9729315005810908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████▍                | 119/200 [00:12<00:07, 11.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 120, loss: 0.2242017239332199\n",
      "AUC Score ->  0.9707251252650164\n",
      "AP Score  ->  0.9742070015898018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████████████████████████▊              | 131/200 [00:14<00:08,  8.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 130, loss: 0.22203992307186127\n",
      "AUC Score ->  0.9712476501305483\n",
      "AP Score  ->  0.9746051943353633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|████████████████████████████▉            | 141/200 [00:15<00:07,  7.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 140, loss: 0.21795400977134705\n",
      "AUC Score ->  0.9719706494692855\n",
      "AP Score  ->  0.9751982052691142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|██████████████████████████████▉          | 151/200 [00:16<00:05,  8.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 150, loss: 0.21605145931243896\n",
      "AUC Score ->  0.9723762333917336\n",
      "AP Score  ->  0.9754768248381445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████▊        | 160/200 [00:17<00:04,  9.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 160, loss: 0.21587936580181122\n",
      "AUC Score ->  0.9728457597365856\n",
      "AP Score  ->  0.9758021572851515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|██████████████████████████████████▊      | 170/200 [00:18<00:02, 11.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 170, loss: 0.21081021428108215\n",
      "AUC Score ->  0.9735773865115993\n",
      "AP Score  ->  0.97644273189574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████▉    | 180/200 [00:19<00:01, 10.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 180, loss: 0.21041707694530487\n",
      "AUC Score ->  0.9737635606623537\n",
      "AP Score  ->  0.9766473246730882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|██████████████████████████████████████▉  | 190/200 [00:20<00:00, 10.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 190, loss: 0.20679619908332825\n",
      "AUC Score ->  0.9745665574787477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|███████████████████████████████████████▎ | 192/200 [00:20<00:01,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP Score  ->  0.9772354546070432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 200/200 [00:21<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC =  0.9645492760261468\n",
      "Test AP  =  0.9699718507665834\n"
     ]
    }
   ],
   "source": [
    "linear_pred_edges = torch.nn.Sequential(torch.nn.Linear(EMBEDDING_DIMENSION,1), torch.nn.Sigmoid())\n",
    "word_embeddings = train_edge_classification(nc_model,linear_pred_edges, graph_dataset, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9344, 64])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7975, 0.0000, 0.4177, 0.6892, 1.1205, 0.0000, 0.9303, 1.1190, 2.3185,\n",
       "        0.0000, 0.8899, 0.0000, 0.0000, 0.0000, 0.1875, 0.0000, 0.6177, 0.0000,\n",
       "        0.0000, 0.7564, 0.9341, 0.0000, 1.9227, 0.0000, 1.3086, 1.3907, 0.0000,\n",
       "        0.0000, 1.3917, 0.3986, 0.0000, 0.0000, 0.6023, 0.9117, 0.0000, 0.9524,\n",
       "        0.6193, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9112, 0.6968, 1.1294,\n",
       "        0.4323, 0.0000, 0.0000, 0.8983, 0.9505, 0.0000, 0.4560, 0.1150, 0.7290,\n",
       "        0.1269, 0.2383, 0.0000, 0.0942, 0.0000, 1.6153, 0.8349, 0.0000, 0.1989,\n",
       "        0.9258], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the Word Embeddings from GCN for LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create n-gram sequences from the original corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_gram_sequences = []\n",
    "two_gram_sequences = []\n",
    "three_gram_sequences = []\n",
    "five_gram_sequences = []\n",
    "ten_gram_sequences = []\n",
    "processed_corpus_tokens = processed_corpus.split(\" \")\n",
    "\n",
    "corpus_with_indices = [word_index[w] for w in processed_corpus_tokens]\n",
    "\n",
    "for idx, word in enumerate(corpus_with_indices):\n",
    "    if idx+11 < len(processed_corpus_tokens):\n",
    "        one_gram_sequences.append(processed_corpus_tokens[idx:idx+2])\n",
    "        two_gram_sequences.append(processed_corpus_tokens[idx:idx+3])\n",
    "        three_gram_sequences.append(processed_corpus_tokens[idx:idx+4])\n",
    "        five_gram_sequences.append(processed_corpus_tokens[idx:idx+6])\n",
    "        ten_gram_sequences.append(processed_corpus_tokens[idx:idx+11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_array = np.zeros((EMBEDDING_DIMENSION))\n",
    "# zero_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_array = np.zeros((EMBEDDING_DIMENSION))\n",
    "def word_seq_to_padded_embeddings(word_seq, word_index, max_seq_length=3):\n",
    "    index_seq = []\n",
    "    for seq in word_seq:\n",
    "        embedding_list = []\n",
    "        additional_zero_pads = max_seq_length - len(seq)\n",
    "        for word in seq:\n",
    "            idx = word_index[word]\n",
    "            embedding_list.append(word_embeddings[idx].cpu().detach().numpy())\n",
    "        for i in range(additional_zero_pads):\n",
    "            embedding_list.append(zero_array)\n",
    "        index_seq.append(np.array(embedding_list))\n",
    "    return index_seq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='uint8')[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "X.extend(word_seq_to_padded_embeddings(np.array(one_gram_sequences)[:,:-1], word_index, 10))\n",
    "y_one_gram_indices = [word_index[word] for word in np.array(one_gram_sequences)[:,-1]]\n",
    "y_one_gram_onehot = to_categorical(y_one_gram_indices, len(unique_words))\n",
    "Y.extend(y_one_gram_onehot)\n",
    "\n",
    "X.extend(word_seq_to_padded_embeddings(np.array(two_gram_sequences)[:,:-1], word_index, 10))\n",
    "y_two_gram_indices = [word_index[word] for word in np.array(two_gram_sequences)[:,-1]]\n",
    "y_two_gram_onehot = to_categorical(y_two_gram_indices, len(unique_words))\n",
    "Y.extend(y_two_gram_onehot)\n",
    "\n",
    "X.extend(word_seq_to_padded_embeddings(np.array(three_gram_sequences)[:,:-1], word_index, 10))\n",
    "y_three_gram_indices = [word_index[word] for word in np.array(three_gram_sequences)[:,-1]]\n",
    "y_three_gram_onehot = to_categorical(y_three_gram_indices, len(unique_words))\n",
    "Y.extend(y_three_gram_onehot)\n",
    "\n",
    "X.extend(word_seq_to_padded_embeddings(np.array(five_gram_sequences)[:,:-1], word_index, 10))\n",
    "y_five_gram_indices = [word_index[word] for word in np.array(five_gram_sequences)[:,-1]]\n",
    "y_five_gram_onehot = to_categorical(y_five_gram_indices, len(unique_words))\n",
    "Y.extend(y_five_gram_onehot)\n",
    "\n",
    "X.extend(word_seq_to_padded_embeddings(np.array(ten_gram_sequences)[:,:-1], word_index, 10))\n",
    "y_ten_gram_indices = [word_index[word] for word in np.array(ten_gram_sequences)[:,-1]]\n",
    "y_ten_gram_onehot = to_categorical(y_ten_gram_indices, len(unique_words))\n",
    "Y.extend(y_ten_gram_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382945\n",
      "382945\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create 80/20 split for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_test, Y_test, test_size=0.1, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "train_data = TensorDataset(torch.from_numpy(np.array(X_train)), torch.from_numpy(np.array(Y_train)))\n",
    "val_data = TensorDataset(torch.from_numpy(np.array(X_val)), torch.from_numpy(np.array(Y_val)))\n",
    "test_data = TensorDataset(torch.from_numpy(np.array(X_test)), torch.from_numpy(np.array(Y_test)))\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Next word prediction LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextWordPredModel(torch.nn.Module):\n",
    "    def __init__(self, embedding_dimension, hidden_units, num_classes):\n",
    "        super().__init__()\n",
    "        self.input_size = embedding_dimension\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_classes = num_classes\n",
    "        self.n_layers=2\n",
    "        \n",
    "        self.rnn = torch.nn.LSTM(input_size = self.input_size, hidden_size=self.hidden_units,\n",
    "                                 num_layers=self.n_layers, dropout=0.2, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(self.hidden_units, self.num_classes)\n",
    "        \n",
    "    def forward(self, x, state = None):\n",
    "        h, rnn_state = self.rnn(x, state)\n",
    "        h = self.fc(h)\n",
    "        scores = F.log_softmax(h, dim=1)\n",
    "        return scores[:,-1,:], rnn_state\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_units).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_units).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs, lr=0.005):\n",
    "    cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    model.train()\n",
    "    for ep in tqdm(range(epochs)):\n",
    "        init_state = model.init_hidden(batch_size)\n",
    "        train_losses=[]\n",
    "        num_correct = 0\n",
    "        total = 0\n",
    "        h = tuple([e.data for e in init_state])\n",
    "        for idx, (inp_seq, op_word) in enumerate(train_loader):\n",
    "            \n",
    "            inp_seq, op_word = inp_seq.float().to(device), op_word.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            prediction, (h_state, c_state) = model(inp_seq, h)\n",
    "            h = (h_state.detach(), c_state.detach())\n",
    "            \n",
    "            loss = cross_entropy(prediction, op_word)\n",
    "            num_correct += prediction.argmax(dim=1).eq(op_word.argmax(dim=1)).sum().item()\n",
    "            total += op_word.shape[0]\n",
    "            train_losses.append(loss.item())\n",
    "    #         print(\"Loss = \", loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if (ep+1)%10 == 0:\n",
    "            val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            val_num_correct = 0\n",
    "            val_total = 0\n",
    "            model.eval()\n",
    "            val_h = tuple([each.data for each in val_h])\n",
    "            \n",
    "            for inp, lab in val_loader:\n",
    "                \n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, (val_h_state, val_c_state) = model(inp.float(), val_h)\n",
    "                val_h = (val_h_state.detach(), val_c_state.detach())\n",
    "                \n",
    "                val_num_correct += out.argmax(dim=1).eq(lab.argmax(dim=1)).sum().item()\n",
    "                val_total += lab.shape[0]\n",
    "                val_loss = cross_entropy(out, lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            if (num_correct >0):\n",
    "                print(\"At least one correct :: num_correct = \", num_correct)\n",
    "            train_acc = (num_correct / total)\n",
    "            val_acc = (val_num_correct / val_total)\n",
    "#             print(train_losses)\n",
    "            print(\"Epoch: {}/{}...\".format(ep+1, epochs),\n",
    "                  \"Step: {}...\".format(idx),\n",
    "                  \"Train Loss: {:.6f}...\".format(np.mean(train_losses)),\n",
    "                  \"Train Acc: {:.3f}...\".format(train_acc),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
    "                  \"Val Acc: {:.3f}...\".format(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▊                                       | 10/500 [02:55<2:26:53, 17.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  33687\n",
      "Epoch: 10/500... Step: 2298... Train Loss: 4.827836... Train Acc: 0.147... Val Loss: 5.064383 Val Acc: 0.148...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▌                                      | 20/500 [05:54<2:26:23, 18.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  54679\n",
      "Epoch: 20/500... Step: 2298... Train Loss: 3.775284... Train Acc: 0.238... Val Loss: 4.421593 Val Acc: 0.202...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▍                                     | 30/500 [08:54<2:24:02, 18.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  71063\n",
      "Epoch: 30/500... Step: 2298... Train Loss: 3.219056... Train Acc: 0.309... Val Loss: 4.164583 Val Acc: 0.232...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███▏                                    | 40/500 [11:55<2:21:33, 18.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  82759\n",
      "Epoch: 40/500... Step: 2298... Train Loss: 2.880222... Train Acc: 0.360... Val Loss: 4.062173 Val Acc: 0.248...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████                                    | 50/500 [14:56<2:18:28, 18.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  91541\n",
      "Epoch: 50/500... Step: 2298... Train Loss: 2.650597... Train Acc: 0.398... Val Loss: 4.001685 Val Acc: 0.263...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████▊                                   | 60/500 [17:58<2:15:58, 18.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  97573\n",
      "Epoch: 60/500... Step: 2298... Train Loss: 2.487294... Train Acc: 0.424... Val Loss: 3.983474 Val Acc: 0.274...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████▌                                  | 70/500 [20:59<2:12:18, 18.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  102959\n",
      "Epoch: 70/500... Step: 2298... Train Loss: 2.365567... Train Acc: 0.448... Val Loss: 3.979365 Val Acc: 0.282...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████▍                                 | 80/500 [24:00<2:08:42, 18.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  106976\n",
      "Epoch: 80/500... Step: 2298... Train Loss: 2.269219... Train Acc: 0.465... Val Loss: 3.975545 Val Acc: 0.290...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████▏                                | 90/500 [27:00<2:05:14, 18.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  110627\n",
      "Epoch: 90/500... Step: 2298... Train Loss: 2.189664... Train Acc: 0.481... Val Loss: 3.995829 Val Acc: 0.295...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████▊                               | 100/500 [30:00<2:02:15, 18.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  113262\n",
      "Epoch: 100/500... Step: 2298... Train Loss: 2.127627... Train Acc: 0.493... Val Loss: 4.004541 Val Acc: 0.299...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████▌                              | 110/500 [32:59<1:58:38, 18.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  115413\n",
      "Epoch: 110/500... Step: 2298... Train Loss: 2.074093... Train Acc: 0.502... Val Loss: 4.037704 Val Acc: 0.301...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████▎                             | 120/500 [35:58<1:55:48, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  117846\n",
      "Epoch: 120/500... Step: 2298... Train Loss: 2.027914... Train Acc: 0.513... Val Loss: 4.062194 Val Acc: 0.302...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██████████▏                            | 130/500 [38:57<1:52:19, 18.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  119491\n",
      "Epoch: 130/500... Step: 2298... Train Loss: 1.988959... Train Acc: 0.520... Val Loss: 4.072759 Val Acc: 0.308...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████▉                            | 140/500 [41:56<1:49:33, 18.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  120589\n",
      "Epoch: 140/500... Step: 2298... Train Loss: 1.957577... Train Acc: 0.525... Val Loss: 4.092851 Val Acc: 0.307...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████▋                           | 150/500 [44:55<1:46:18, 18.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  121945\n",
      "Epoch: 150/500... Step: 2298... Train Loss: 1.925914... Train Acc: 0.530... Val Loss: 4.107124 Val Acc: 0.312...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|████████████▍                          | 160/500 [47:54<1:43:13, 18.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  123763\n",
      "Epoch: 160/500... Step: 2298... Train Loss: 1.892598... Train Acc: 0.538... Val Loss: 4.141226 Val Acc: 0.314...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|█████████████▎                         | 170/500 [50:53<1:40:41, 18.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  124753\n",
      "Epoch: 170/500... Step: 2298... Train Loss: 1.869160... Train Acc: 0.543... Val Loss: 4.173375 Val Acc: 0.316...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|██████████████                         | 180/500 [53:53<1:37:43, 18.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  126158\n",
      "Epoch: 180/500... Step: 2298... Train Loss: 1.846999... Train Acc: 0.549... Val Loss: 4.184699 Val Acc: 0.315...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████████▊                        | 190/500 [56:52<1:34:45, 18.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  126725\n",
      "Epoch: 190/500... Step: 2298... Train Loss: 1.829384... Train Acc: 0.551... Val Loss: 4.207950 Val Acc: 0.316...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████▌                       | 200/500 [59:53<1:32:05, 18.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  127534\n",
      "Epoch: 200/500... Step: 2298... Train Loss: 1.812188... Train Acc: 0.555... Val Loss: 4.219138 Val Acc: 0.317...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|███████████████▌                     | 210/500 [1:02:54<1:29:06, 18.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  128431\n",
      "Epoch: 210/500... Step: 2298... Train Loss: 1.794707... Train Acc: 0.559... Val Loss: 4.232137 Val Acc: 0.318...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████████████████▎                    | 220/500 [1:05:55<1:26:20, 18.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  129913\n",
      "Epoch: 220/500... Step: 2298... Train Loss: 1.770338... Train Acc: 0.565... Val Loss: 4.266524 Val Acc: 0.318...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|█████████████████                    | 230/500 [1:08:57<1:23:11, 18.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  130267\n",
      "Epoch: 230/500... Step: 2298... Train Loss: 1.762234... Train Acc: 0.567... Val Loss: 4.266339 Val Acc: 0.321...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|█████████████████▊                   | 240/500 [1:11:58<1:20:11, 18.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  131088\n",
      "Epoch: 240/500... Step: 2298... Train Loss: 1.741728... Train Acc: 0.570... Val Loss: 4.285909 Val Acc: 0.324...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████▌                  | 250/500 [1:14:59<1:16:59, 18.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  131390\n",
      "Epoch: 250/500... Step: 2298... Train Loss: 1.731206... Train Acc: 0.572... Val Loss: 4.325960 Val Acc: 0.322...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|███████████████████▏                 | 260/500 [1:18:01<1:14:08, 18.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  132195\n",
      "Epoch: 260/500... Step: 2298... Train Loss: 1.718839... Train Acc: 0.575... Val Loss: 4.320296 Val Acc: 0.323...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|███████████████████▉                 | 270/500 [1:21:02<1:10:44, 18.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  132988\n",
      "Epoch: 270/500... Step: 2298... Train Loss: 1.703533... Train Acc: 0.578... Val Loss: 4.326797 Val Acc: 0.320...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|████████████████████▋                | 280/500 [1:24:04<1:08:02, 18.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  132860\n",
      "Epoch: 280/500... Step: 2298... Train Loss: 1.702639... Train Acc: 0.578... Val Loss: 4.345622 Val Acc: 0.326...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████████████████████▍               | 290/500 [1:27:06<1:04:49, 18.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  133500\n",
      "Epoch: 290/500... Step: 2298... Train Loss: 1.689760... Train Acc: 0.581... Val Loss: 4.378185 Val Acc: 0.325...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████▏              | 300/500 [1:30:08<1:01:52, 18.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  134334\n",
      "Epoch: 300/500... Step: 2298... Train Loss: 1.676381... Train Acc: 0.584... Val Loss: 4.391687 Val Acc: 0.323...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|████████████████████████▏              | 310/500 [1:33:09<58:27, 18.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  134763\n",
      "Epoch: 310/500... Step: 2298... Train Loss: 1.668475... Train Acc: 0.586... Val Loss: 4.403092 Val Acc: 0.325...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████▉              | 320/500 [1:36:11<55:32, 18.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  135248\n",
      "Epoch: 320/500... Step: 2298... Train Loss: 1.656698... Train Acc: 0.588... Val Loss: 4.411882 Val Acc: 0.324...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|█████████████████████████▋             | 330/500 [1:39:12<52:24, 18.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  135917\n",
      "Epoch: 330/500... Step: 2298... Train Loss: 1.643023... Train Acc: 0.591... Val Loss: 4.444364 Val Acc: 0.328...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████████████████████████▌            | 340/500 [1:42:14<49:27, 18.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  136140\n",
      "Epoch: 340/500... Step: 2298... Train Loss: 1.643336... Train Acc: 0.592... Val Loss: 4.441659 Val Acc: 0.327...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████████████████████████▎           | 350/500 [1:45:16<46:20, 18.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  136744\n",
      "Epoch: 350/500... Step: 2298... Train Loss: 1.631197... Train Acc: 0.595... Val Loss: 4.445152 Val Acc: 0.326...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|████████████████████████████           | 360/500 [1:48:18<43:09, 18.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  136841\n",
      "Epoch: 360/500... Step: 2298... Train Loss: 1.626862... Train Acc: 0.595... Val Loss: 4.465076 Val Acc: 0.328...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|████████████████████████████▊          | 370/500 [1:51:18<39:49, 18.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  137032\n",
      "Epoch: 370/500... Step: 2298... Train Loss: 1.618307... Train Acc: 0.596... Val Loss: 4.470961 Val Acc: 0.330...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|█████████████████████████████▋         | 380/500 [1:54:18<36:42, 18.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  137806\n",
      "Epoch: 380/500... Step: 2298... Train Loss: 1.606089... Train Acc: 0.599... Val Loss: 4.501866 Val Acc: 0.329...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|██████████████████████████████▍        | 390/500 [1:57:18<33:27, 18.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  138113\n",
      "Epoch: 390/500... Step: 2298... Train Loss: 1.602644... Train Acc: 0.601... Val Loss: 4.489742 Val Acc: 0.330...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████▏       | 400/500 [2:00:18<30:33, 18.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  138297\n",
      "Epoch: 400/500... Step: 2298... Train Loss: 1.596902... Train Acc: 0.602... Val Loss: 4.518561 Val Acc: 0.331...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|███████████████████████████████▉       | 410/500 [2:03:17<27:21, 18.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  138321\n",
      "Epoch: 410/500... Step: 2298... Train Loss: 1.591284... Train Acc: 0.602... Val Loss: 4.522983 Val Acc: 0.329...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████████████████████████████▊      | 420/500 [2:06:16<24:22, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  138938\n",
      "Epoch: 420/500... Step: 2298... Train Loss: 1.584429... Train Acc: 0.604... Val Loss: 4.539976 Val Acc: 0.331...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|█████████████████████████████████▌     | 430/500 [2:09:15<21:14, 18.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  139355\n",
      "Epoch: 430/500... Step: 2298... Train Loss: 1.577017... Train Acc: 0.606... Val Loss: 4.535849 Val Acc: 0.332...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|██████████████████████████████████▎    | 440/500 [2:12:14<18:16, 18.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  139397\n",
      "Epoch: 440/500... Step: 2298... Train Loss: 1.574910... Train Acc: 0.606... Val Loss: 4.564084 Val Acc: 0.330...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████    | 450/500 [2:15:13<15:13, 18.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  139844\n",
      "Epoch: 450/500... Step: 2298... Train Loss: 1.567097... Train Acc: 0.608... Val Loss: 4.551934 Val Acc: 0.333...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████▉   | 460/500 [2:18:13<12:15, 18.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  139903\n",
      "Epoch: 460/500... Step: 2298... Train Loss: 1.565761... Train Acc: 0.609... Val Loss: 4.593003 Val Acc: 0.332...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|████████████████████████████████████▋  | 470/500 [2:21:14<09:11, 18.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  140709\n",
      "Epoch: 470/500... Step: 2298... Train Loss: 1.552564... Train Acc: 0.612... Val Loss: 4.595441 Val Acc: 0.332...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████████████████████████████████▍ | 480/500 [2:24:15<06:09, 18.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  140884\n",
      "Epoch: 480/500... Step: 2298... Train Loss: 1.550345... Train Acc: 0.613... Val Loss: 4.600771 Val Acc: 0.331...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██████████████████████████████████████▏| 490/500 [2:27:16<03:04, 18.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  140987\n",
      "Epoch: 490/500... Step: 2298... Train Loss: 1.547027... Train Acc: 0.613... Val Loss: 4.597930 Val Acc: 0.333...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [2:30:17<00:00, 18.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  140968\n",
      "Epoch: 500/500... Step: 2298... Train Loss: 1.543993... Train Acc: 0.613... Val Loss: 4.619086 Val Acc: 0.332...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training block for celebrity corpus\n",
    "# Train Accuracy = 61.3% Val Accuracy = 33.2%\n",
    "# model = NextWordPredModel(embedding_dimension=EMBEDDING_DIMENSION, hidden_units=256, num_classes=len(unique_words))\n",
    "# train_model(model.to(device), 500, 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▊                                       | 10/500 [04:19<3:37:34, 26.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  40176\n",
      "Epoch: 10/500... Step: 3062... Train Loss: 5.045858... Train Acc: 0.131... Val Loss: 5.305742 Val Acc: 0.133...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▌                                      | 20/500 [08:45<3:37:07, 27.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  60930\n",
      "Epoch: 20/500... Step: 3062... Train Loss: 4.159292... Train Acc: 0.199... Val Loss: 4.822711 Val Acc: 0.168...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▍                                     | 30/500 [13:13<3:33:40, 27.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  78152\n",
      "Epoch: 30/500... Step: 3062... Train Loss: 3.677666... Train Acc: 0.255... Val Loss: 4.624567 Val Acc: 0.191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███▏                                    | 40/500 [17:41<3:29:32, 27.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  90309\n",
      "Epoch: 40/500... Step: 3062... Train Loss: 3.371451... Train Acc: 0.295... Val Loss: 4.532623 Val Acc: 0.209...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████                                    | 50/500 [22:09<3:24:35, 27.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  99356\n",
      "Epoch: 50/500... Step: 3062... Train Loss: 3.162378... Train Acc: 0.324... Val Loss: 4.490783 Val Acc: 0.219...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████▊                                   | 60/500 [26:38<3:20:30, 27.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  106594\n",
      "Epoch: 60/500... Step: 3062... Train Loss: 3.006875... Train Acc: 0.348... Val Loss: 4.451553 Val Acc: 0.228...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████▌                                  | 70/500 [31:06<3:15:26, 27.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  112228\n",
      "Epoch: 70/500... Step: 3062... Train Loss: 2.891007... Train Acc: 0.366... Val Loss: 4.436703 Val Acc: 0.235...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████▍                                 | 80/500 [35:36<3:12:08, 27.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  117363\n",
      "Epoch: 80/500... Step: 3062... Train Loss: 2.796609... Train Acc: 0.383... Val Loss: 4.441587 Val Acc: 0.240...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████▏                                | 90/500 [40:05<3:07:01, 27.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  120915\n",
      "Epoch: 90/500... Step: 3062... Train Loss: 2.717921... Train Acc: 0.395... Val Loss: 4.459000 Val Acc: 0.245...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████▊                               | 100/500 [44:34<3:02:57, 27.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  124495\n",
      "Epoch: 100/500... Step: 3062... Train Loss: 2.654053... Train Acc: 0.406... Val Loss: 4.444401 Val Acc: 0.249...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████▌                              | 110/500 [49:07<3:00:27, 27.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  127043\n",
      "Epoch: 110/500... Step: 3062... Train Loss: 2.603478... Train Acc: 0.415... Val Loss: 4.462943 Val Acc: 0.255...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████▎                             | 120/500 [53:41<2:57:09, 27.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  129825\n",
      "Epoch: 120/500... Step: 3062... Train Loss: 2.552301... Train Acc: 0.424... Val Loss: 4.476026 Val Acc: 0.259...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██████████▏                            | 130/500 [58:17<2:53:33, 28.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  132019\n",
      "Epoch: 130/500... Step: 3062... Train Loss: 2.512420... Train Acc: 0.431... Val Loss: 4.479403 Val Acc: 0.263...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████▎                          | 140/500 [1:02:54<2:49:53, 28.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  133914\n",
      "Epoch: 140/500... Step: 3062... Train Loss: 2.475772... Train Acc: 0.437... Val Loss: 4.499234 Val Acc: 0.264...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████                          | 150/500 [1:07:45<2:54:12, 29.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  135577\n",
      "Epoch: 150/500... Step: 3062... Train Loss: 2.449784... Train Acc: 0.443... Val Loss: 4.521499 Val Acc: 0.266...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███████████▊                         | 160/500 [1:12:37<2:48:47, 29.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  137908\n",
      "Epoch: 160/500... Step: 3062... Train Loss: 2.411895... Train Acc: 0.450... Val Loss: 4.531948 Val Acc: 0.270...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|████████████▌                        | 170/500 [1:17:37<2:48:08, 30.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  139494\n",
      "Epoch: 170/500... Step: 3062... Train Loss: 2.386275... Train Acc: 0.455... Val Loss: 4.552912 Val Acc: 0.270...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████▎                       | 180/500 [1:22:34<2:45:17, 30.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  140733\n",
      "Epoch: 180/500... Step: 3062... Train Loss: 2.361122... Train Acc: 0.459... Val Loss: 4.549980 Val Acc: 0.273...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████████                       | 190/500 [1:27:39<2:39:02, 30.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  141919\n",
      "Epoch: 190/500... Step: 3062... Train Loss: 2.340881... Train Acc: 0.463... Val Loss: 4.566618 Val Acc: 0.275...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████▊                      | 200/500 [1:32:34<2:29:54, 29.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  143358\n",
      "Epoch: 200/500... Step: 3062... Train Loss: 2.315273... Train Acc: 0.468... Val Loss: 4.571307 Val Acc: 0.279...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|███████████████▌                     | 210/500 [1:37:18<2:21:13, 29.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  144712\n",
      "Epoch: 210/500... Step: 3062... Train Loss: 2.295955... Train Acc: 0.472... Val Loss: 4.587423 Val Acc: 0.279...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████████████████▎                    | 220/500 [1:41:57<2:12:31, 28.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  145334\n",
      "Epoch: 220/500... Step: 3062... Train Loss: 2.278777... Train Acc: 0.474... Val Loss: 4.586944 Val Acc: 0.283...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|█████████████████                    | 230/500 [1:46:35<2:07:25, 28.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  146695\n",
      "Epoch: 230/500... Step: 3062... Train Loss: 2.260685... Train Acc: 0.479... Val Loss: 4.607208 Val Acc: 0.283...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|█████████████████▊                   | 240/500 [1:51:13<2:02:26, 28.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  147606\n",
      "Epoch: 240/500... Step: 3062... Train Loss: 2.245784... Train Acc: 0.482... Val Loss: 4.608990 Val Acc: 0.284...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████▌                  | 250/500 [1:55:50<1:57:27, 28.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  148404\n",
      "Epoch: 250/500... Step: 3062... Train Loss: 2.232591... Train Acc: 0.485... Val Loss: 4.633955 Val Acc: 0.288...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|███████████████████▏                 | 260/500 [2:00:26<1:52:29, 28.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  149367\n",
      "Epoch: 260/500... Step: 3062... Train Loss: 2.214894... Train Acc: 0.488... Val Loss: 4.646677 Val Acc: 0.288...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|███████████████████▉                 | 270/500 [2:05:04<1:48:14, 28.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  149690\n",
      "Epoch: 270/500... Step: 3062... Train Loss: 2.209621... Train Acc: 0.489... Val Loss: 4.665132 Val Acc: 0.289...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|████████████████████▋                | 280/500 [2:09:40<1:43:00, 28.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  150801\n",
      "Epoch: 280/500... Step: 3062... Train Loss: 2.191867... Train Acc: 0.492... Val Loss: 4.679539 Val Acc: 0.291...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████████████████████▍               | 290/500 [2:14:17<1:38:42, 28.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  151762\n",
      "Epoch: 290/500... Step: 3062... Train Loss: 2.177273... Train Acc: 0.495... Val Loss: 4.696355 Val Acc: 0.293...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████▏              | 300/500 [2:18:53<1:33:51, 28.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  152714\n",
      "Epoch: 300/500... Step: 3062... Train Loss: 2.166719... Train Acc: 0.499... Val Loss: 4.704591 Val Acc: 0.291...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████████████████████▉              | 310/500 [2:23:33<1:31:05, 28.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  153230\n",
      "Epoch: 310/500... Step: 3062... Train Loss: 2.155856... Train Acc: 0.500... Val Loss: 4.701663 Val Acc: 0.294...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|███████████████████████▋             | 320/500 [2:28:17<1:26:27, 28.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  153482\n",
      "Epoch: 320/500... Step: 3062... Train Loss: 2.144790... Train Acc: 0.501... Val Loss: 4.714164 Val Acc: 0.296...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|████████████████████████▍            | 330/500 [2:33:13<1:24:46, 29.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  154307\n",
      "Epoch: 330/500... Step: 3062... Train Loss: 2.135937... Train Acc: 0.504... Val Loss: 4.722053 Val Acc: 0.297...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|█████████████████████████▏           | 340/500 [2:38:18<1:23:28, 31.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  154532\n",
      "Epoch: 340/500... Step: 3062... Train Loss: 2.127518... Train Acc: 0.505... Val Loss: 4.749752 Val Acc: 0.296...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████▉           | 350/500 [2:43:27<1:18:25, 31.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  155484\n",
      "Epoch: 350/500... Step: 3062... Train Loss: 2.117427... Train Acc: 0.508... Val Loss: 4.747963 Val Acc: 0.299...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|██████████████████████████▋          | 360/500 [2:48:39<1:14:12, 31.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  156005\n",
      "Epoch: 360/500... Step: 3062... Train Loss: 2.108857... Train Acc: 0.509... Val Loss: 4.756731 Val Acc: 0.301...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████████████████████████▍         | 370/500 [2:53:48<1:07:19, 31.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  156679\n",
      "Epoch: 370/500... Step: 3062... Train Loss: 2.097504... Train Acc: 0.512... Val Loss: 4.749269 Val Acc: 0.301...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|█████████████████████████████▋         | 380/500 [2:58:39<59:18, 29.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  157298\n",
      "Epoch: 380/500... Step: 3062... Train Loss: 2.091279... Train Acc: 0.514... Val Loss: 4.788580 Val Acc: 0.297...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|██████████████████████████████▍        | 390/500 [3:03:33<54:21, 29.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  157594\n",
      "Epoch: 390/500... Step: 3062... Train Loss: 2.084248... Train Acc: 0.515... Val Loss: 4.789634 Val Acc: 0.302...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████▏       | 400/500 [3:08:22<49:20, 29.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  157921\n",
      "Epoch: 400/500... Step: 3062... Train Loss: 2.073718... Train Acc: 0.516... Val Loss: 4.800413 Val Acc: 0.304...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|███████████████████████████████▉       | 410/500 [3:13:12<44:07, 29.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  158377\n",
      "Epoch: 410/500... Step: 3062... Train Loss: 2.066575... Train Acc: 0.517... Val Loss: 4.800718 Val Acc: 0.305...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████████████████████████████▊      | 420/500 [3:17:53<39:08, 29.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  158809\n",
      "Epoch: 420/500... Step: 3062... Train Loss: 2.063128... Train Acc: 0.518... Val Loss: 4.825942 Val Acc: 0.305...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|█████████████████████████████████▌     | 430/500 [3:22:37<33:18, 28.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  159381\n",
      "Epoch: 430/500... Step: 3062... Train Loss: 2.057430... Train Acc: 0.520... Val Loss: 4.833952 Val Acc: 0.305...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|██████████████████████████████████▎    | 440/500 [3:27:23<29:42, 29.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  158354\n",
      "Epoch: 440/500... Step: 3062... Train Loss: 2.069957... Train Acc: 0.517... Val Loss: 4.834384 Val Acc: 0.305...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████    | 450/500 [3:32:06<23:50, 28.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  159733\n",
      "Epoch: 450/500... Step: 3062... Train Loss: 2.048972... Train Acc: 0.521... Val Loss: 4.839302 Val Acc: 0.308...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████▉   | 460/500 [3:36:50<19:30, 29.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  160405\n",
      "Epoch: 460/500... Step: 3062... Train Loss: 2.038199... Train Acc: 0.524... Val Loss: 4.856903 Val Acc: 0.306...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|████████████████████████████████████▋  | 470/500 [3:41:33<14:46, 29.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  160913\n",
      "Epoch: 470/500... Step: 3062... Train Loss: 2.032770... Train Acc: 0.525... Val Loss: 4.872825 Val Acc: 0.305...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████████████████████████████████▍ | 480/500 [3:46:23<10:04, 30.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  161122\n",
      "Epoch: 480/500... Step: 3062... Train Loss: 2.026696... Train Acc: 0.526... Val Loss: 4.873804 Val Acc: 0.307...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██████████████████████████████████████▏| 490/500 [3:51:25<05:09, 30.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  161804\n",
      "Epoch: 490/500... Step: 3062... Train Loss: 2.015396... Train Acc: 0.528... Val Loss: 4.895746 Val Acc: 0.309...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [3:56:31<00:00, 28.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one correct :: num_correct =  161938\n",
      "Epoch: 500/500... Step: 3062... Train Loss: 2.010200... Train Acc: 0.529... Val Loss: 4.889572 Val Acc: 0.308...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Block for Music corpus\n",
    "model = NextWordPredModel(embedding_dimension=EMBEDDING_DIMENSION, hidden_units=256, num_classes=len(unique_words))\n",
    "train_model(model.to(device), 500, 0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy =  0.309\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "total = 0\n",
    "h = model.init_hidden(batch_size)\n",
    "\n",
    "model.eval()\n",
    "h = tuple([each.data for each in h])\n",
    "for inputs, labels in test_loader:\n",
    "    \n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    output, (test_h_state, test_c_state) = model(inputs.float(), h)\n",
    "    h = (test_h_state.detach(), test_c_state.detach())\n",
    "\n",
    "    pred = output.argmax(dim=1)\n",
    "    correct = pred.eq(labels.argmax(dim=1)).sum().item()\n",
    "\n",
    "    num_correct += correct\n",
    "    total += inputs.shape[0]\n",
    "test_acc = round(num_correct/total, 3)\n",
    "print(\"Test Accuracy = \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save checkpoints for the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now().strftime(\"%Y-%m-%d::%H:%M:%S\")\n",
    "\n",
    "PATH = 'model-' + keyword + \"-\" + str(len(X)) + \"-\" + now + \"-\" + str(test_acc) + \".pt\"\n",
    "\n",
    "torch.save({\n",
    "            'model_state_dict': model.state_dict()\n",
    "            }, PATH)\n",
    "pickle.dump(word_embeddings, open(\"word_embeddings_\" + keyword + \"_\" + now + \".pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = word_seq_to_padded_embeddings(np.array(word_tokens), word_index, 5).unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD TRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NextWordPredModel(\n",
       "  (rnn): LSTM(64, 256, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (fc): Linear(in_features=256, out_features=7901, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH = \"model-287430-2022-11-21::10:22:40-0.322\"\n",
    "word_embeddings = pickle.load(open(\"word_embeddings_2022-11-21::10:22:40.pk\", \"rb\"))\n",
    "\n",
    "model = NextWordPredModel(embedding_dimension=EMBEDDING_DIMENSION, hidden_units=256, num_classes=len(unique_words))\n",
    "checkpoint = torch.load(MODEL_PATH)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST THE TRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(word_model, sentence):\n",
    "    words = sentence.strip().split()\n",
    "    word_tokens = []\n",
    "    for w in words:\n",
    "        if w in word_index:\n",
    "            word_tokens.append(word_index[w])\n",
    "    \n",
    "    input_sequence = word_embeddings[word_tokens].unsqueeze(dim=0)\n",
    "    prediction, _ = word_model(input_sequence.to(device))\n",
    "    predicted_wordindex = prediction.argmax(dim=1).item()\n",
    "    predicted_word = index_word[predicted_wordindex]\n",
    "    return predicted_word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' styles of new mexico red dirt tejano and texas country its popularized roots originate in the southern and southwestern united states of the early 1920s country music often consists of ballads and honky tonk dance tunes with generally simple form folk lyrics and harmonies often accompanied by string instruments such as electric and acoustic guitars steel guitars such as pedal steels and dobros banjos and fiddles as well as harmonicas blues modes have been used extensively throughout its recorde'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_corpus[500:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Word prediction =  ownership\n"
     ]
    }
   ],
   "source": [
    "pred_word = predict_next_word(model.to(device), \"the celebrities arrived at the\")\n",
    "print(\"Next Word prediction = \", pred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Word prediction =  spontaneously\n"
     ]
    }
   ],
   "source": [
    "pred_word = predict_next_word(model.to(device), \"the guitar player was\")\n",
    "print(\"Next Word prediction = \", pred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Word prediction =  depends\n"
     ]
    }
   ],
   "source": [
    "pred_word = predict_next_word(model.to(device), \"the song browser\")\n",
    "print(\"Next Word prediction = \", pred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Word prediction =  opeth\n"
     ]
    }
   ],
   "source": [
    "pred_word = predict_next_word(model.to(device), \"rock music is rated very high in\")\n",
    "print(\"Next Word prediction = \", pred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the main man was lyons deeper lighter frostbite maximum strong and minute hiatus hawaiian\n"
     ]
    }
   ],
   "source": [
    "sentence = \"the main man was\"\n",
    "for i in range(10):\n",
    "    predct = predict_next_word(model, sentence)\n",
    "    sentence = sentence + \" \" + predct\n",
    "print(sentence)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
